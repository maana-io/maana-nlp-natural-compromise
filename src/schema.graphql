# import Post from "./generated/prisma.graphql"
enum SUPPORTED_TOKENIZERS {
  WordTokenizer
  WordPunctTokenizer
  SentenceTokenizer
}

enum NORMALIZATION_OPTIONS {
  whitespace
  case
  numbers
  punctuation
  unicode
  contractions
  acronyms
  parentheses
  possessives
  plurals
  verbs
  honorifics
}

type RankedWord {
  word: String
  count: Float
}

input RankedWordInput {
  word: String
  count: Float
}

type Query {
  me: User

  # Compromise
  nouns(texts: [String!]!): [String!]!
  nounsTags(texts: [String!]!): [Tag!]!

  verbs(texts: [String!]!): [String!]!
  verbsTags(texts: [String!]!): [Tag!]!
  values(texts: [String!]!): [String!]!
  valuesTags(texts: [String!]!): [Tag!]!

  sentences(texts: [String!]!): [String!]!
  sentencesTags(texts: [String!]!): [Tag!]!

  contractions(texts: [String!]!): [String!]!
  contractionsTags(texts: [String!]!): [Tag!]!

  compromiseMatch(text: String, expression: String): [String!]!

  people(texts: [String!]!): [String!]!
  peopleTags(texts: [String!]!): [Tag!]!

  places(texts: [String!]!): [String!]!
  placesTags(texts: [String!]!): [Tag!]!

  organizations(texts: [String!]!): [String!]!
  organizationsTags(texts: [String!]!): [Tag!]!

  topics(texts: [String!]!): [String!]!
  topicsTags(texts: [String!]!): [Tag!]!

  normalize(text: String, options: [NORMALIZATION_OPTIONS!]): String

  # Natural
  tokenize(texts: [String!]!, tokenizerType: SUPPORTED_TOKENIZERS): [String]

  ## String Distances

  hammingDistance(source: String, target: String): Int
  jaroWinklerDistance(source: String, target: String): Int
  levenshteinDistance(
    source: String
    target: String
    insertionCost: Int
    deletionCost: Int
    substitutionCost: Int
  ): Int
  damerauLevenshteinDistance(
    source: String
    target: String
    transportationCost: Int
    restricted: Boolean
  ): Int
  diceCoefficient(source: String, target: String): Float

  ## Classififers

  classify(classifierName: String, text: String): [Classification]

  ## Sentiment

  sentiment(texts: [String!]!): [String!]!
  average(values: [String!]!): String!

  extractKeyword(sentences: [String!]!): [String!]!
  extractKeyphrases(sentences: [String!]!): [String!]!

  getTop10FrequenteWordsInText(text: [String!]!): [String!]!
  getTop100FrequenteRankedWordsInText(text: [String!]!): [RankedWord!]!

  combinedRanked(
    ranked1: [RankedWordInput!]!
    ranked2: [RankedWordInput!]!
  ): [RankedWord!]!

  combineLists(list1: [String!]!, list2: [String!]!): [String!]!
  getTop10RankedeWords(ranked: [RankedWordInput!]!): [String!]!

  getVocabulary(text: [String!]!): [String!]!
}

type Mutation {
  signup(email: String!, password: String!, name: String!): AuthPayload!
  login(email: String!, password: String!): AuthPayload!

  createClassifier(name: String!): Classifier
  addLabeledDocumentToClassifier(
    classifierName: String
    text: String
    label: String
  ): Classifier
}

type Subscription {
  feedSubscription: String!
}

type AuthPayload {
  token: String!
  user: User!
}

type User {
  id: ID!
  email: String!
  name: String!
}
